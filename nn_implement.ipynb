{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'ignore'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr(all='raise', under='warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction(abc.ABC):\n",
    "    \"\"\"Base class for activation functions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Init function. Nothing needs to do in the base class.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def derivative_use_activated(self):\n",
    "        \"\"\"Boolean: If derivative of the activation function can be calculated\n",
    "        using activated value.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def apply(self, v):\n",
    "        \"\"\"Apply activation function on the data.\n",
    "        \n",
    "        Args:\n",
    "            v (np.array): Data\n",
    "        \n",
    "        Returns:\n",
    "            Activated values\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def derivative(self, v):\n",
    "        \"\"\"Calculate derivative wrt input of activation function.\n",
    "        \n",
    "        Args:\n",
    "            v (np.array): input of activation function if derivative_use_activated is false,\n",
    "                output of activation function otherwise\n",
    "        \n",
    "        Returns:\n",
    "            Derivative wrt input of activation function\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationNone(ActivationFunction):\n",
    "    \"\"\"For not using an activation function\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Init function. Do nothing.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def derivative_use_activated(self):\n",
    "        \"\"\"Arbitrary. Not used.\"\"\"\n",
    "        return True\n",
    "    \n",
    "    def apply(self, v):\n",
    "        \"\"\"Return input as output as no activation needs to be applied.\n",
    "        \n",
    "        Args:\n",
    "            v (np.array): Data\n",
    "        \n",
    "        Returns:\n",
    "            The same as input v\n",
    "        \"\"\"\n",
    "        return v\n",
    "    \n",
    "    def derivative(self, v):\n",
    "        \"\"\"Calculate derivative wrt input of activation function.\n",
    "        \n",
    "        Args:\n",
    "            v (np.array): input of activation function if derivative_use_activated is false,\n",
    "                output of activation function otherwise\n",
    "        \n",
    "        Returns:\n",
    "            Always 1\n",
    "        \"\"\"\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationSigmoid(ActivationFunction):\n",
    "    \"\"\"Sigmoid function. Data out of bounds will be clipped.\"\"\"\n",
    "    \n",
    "    def __init__(self, x_upper_bound=None, x_lower_bound=None):\n",
    "        \"\"\"Init function.\n",
    "        \n",
    "        Args:\n",
    "            x_upper_bound (int): Upper bound of sigmoid input\n",
    "            x_lower_bound (int): Lower bound of sigmoid input\n",
    "        \"\"\"\n",
    "        self.x_upper_bound = x_upper_bound\n",
    "        self.x_lower_bound = x_lower_bound\n",
    "    \n",
    "    @property\n",
    "    def derivative_use_activated(self):\n",
    "        \"\"\"Use sigmoid result to calculate derivative.\"\"\"\n",
    "        return True\n",
    "    \n",
    "    def apply(self, v):\n",
    "        \"\"\"Apply sigmoid function to data.\n",
    "        \n",
    "        Args:\n",
    "            v (np.array): Data\n",
    "        \n",
    "        Returns:\n",
    "            1 / (1 + exp(v)). v will be clipped if bounds are set.\n",
    "        \"\"\"\n",
    "        if self.x_upper_bound or self.x_upper_bound:\n",
    "            return 1.0 / (1.0 + np.exp(-v.clip(max=self.x_upper_bound, min=self.x_lower_bound)))\n",
    "        return 1.0 / (1.0 + np.exp(-v))\n",
    "    \n",
    "    def derivative(self, v):\n",
    "        \"\"\"Calculate derivative wrt input of activation function.\n",
    "        \n",
    "        Args:\n",
    "            v (np.array): input of activation function if derivative_use_activated is false,\n",
    "                output of activation function otherwise\n",
    "        \n",
    "        Returns:\n",
    "            v * (1 - v)\n",
    "        \"\"\"\n",
    "        return v * (1 - v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationRelu(ActivationFunction):\n",
    "    \"\"\"Relu function.\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Init function. Do nothing.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def derivative_use_activated(self):\n",
    "        \"\"\"Use result to calculate derivative.\"\"\"\n",
    "        return True\n",
    "    \n",
    "    def apply(self, v):\n",
    "        \"\"\"Apply Relu to data.\n",
    "        \n",
    "        Args:\n",
    "            v (np.array): Data\n",
    "        \n",
    "        Returns:\n",
    "            v if v > 0, otherwise 0\n",
    "        \"\"\"\n",
    "        return v.clip(min=0)\n",
    "    \n",
    "    def derivative(self, v):\n",
    "        \"\"\"Calculate derivative wrt input of activation function.\n",
    "        \n",
    "        Args:\n",
    "            v (np.array): input of activation function if derivative_use_activated is false,\n",
    "                output of activation function otherwise\n",
    "        \n",
    "        Returns:\n",
    "            1 if v > 0, otherwise 0\n",
    "        \"\"\"\n",
    "        return np.where(v>0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gd optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDOptimizer(abc.ABC):\n",
    "    \"\"\"Base class for gradient descent optimizer. One optimizer to N layers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Init function.\n",
    "        \n",
    "        Attributes:\n",
    "            weight_infos (dict): buff to store any step result for weight\n",
    "            intercept_infos (dict): buff to store any step result for intercept\n",
    "        \"\"\"\n",
    "        self.weight_infos = {}\n",
    "        self.intercept_infos = {}\n",
    "    \n",
    "    def register_layer(self, layer_id, weight_shape, intercept_shape):\n",
    "        \"\"\"Register layer which will use this optimizer.\n",
    "        \n",
    "        Args:\n",
    "            layer_id (int/str): key to specify the layer\n",
    "            weight_shape (int): shape of weight of the layer\n",
    "            intercept_shape (int): shape of intercept of the layer\n",
    "        \"\"\"\n",
    "        self.weight_infos[layer_id] = np.zeros(weight_shape)\n",
    "        self.intercept_infos[layer_id] = np.zeros(intercept_shape)\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def apply(self, layer_id, grads, intercept_grads):\n",
    "        \"\"\"Apply optimizer to current gradients for update.\n",
    "        \n",
    "        Args:\n",
    "            layer_id (int/str): key to specify the layer\n",
    "            grads (np.array): gradients of weight\n",
    "            intercept_grads (np.array): gradients of intercept\n",
    "        \n",
    "        Returns:\n",
    "            optimized gradients/update value\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDOptimizerNone(GDOptimizer):\n",
    "    \"\"\"Do no optimization, apply learning rate only.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate):\n",
    "        \"\"\"Init function.\n",
    "        \n",
    "        Attributes:\n",
    "            learning_rate (float): learning rate\n",
    "        \"\"\"\n",
    "        GDOptimizer.__init__(self)\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def apply(self, layer_id, grads, intercept_grads):\n",
    "        \"\"\"Apply optimizer to current gradients for update.\n",
    "        \n",
    "        Args:\n",
    "            layer_id (int/str): key to specify the layer\n",
    "            grads (np.array): gradients of weight\n",
    "            intercept_grads (np.array): gradients of intercept\n",
    "        \n",
    "        Returns:\n",
    "            learning_rate * gradients\n",
    "        \"\"\"\n",
    "        return self.learning_rate * grads, self.learning_rate * intercept_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need a smaller learning rate(0.01?)\n",
    "class GDOptimizerMomentum(GDOptimizer):\n",
    "    \"\"\"Momentum for SGD.\n",
    "    \n",
    "    In Momentum, weight_infos and intercept_infos stores the last calculated weight deltas.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate, gamma=0.9):\n",
    "        \"\"\"Init function.\n",
    "        \n",
    "        Attributes:\n",
    "            learning_rate (float): learning rate\n",
    "            gamma (float): rate of the update vector of the past time step to add on, default 0.9\n",
    "        \"\"\"\n",
    "        GDOptimizer.__init__(self)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def apply(self, layer_id, grads, intercept_grads):\n",
    "        \"\"\"Apply optimizer to current gradients for update.\n",
    "        \n",
    "        Args:\n",
    "            layer_id (int/str): key to specify the layer\n",
    "            grads (np.array): gradients of weight\n",
    "            intercept_grads (np.array): gradients of intercept\n",
    "        \n",
    "        Returns:\n",
    "            gamma * last_gradients + learning_rate * current_gradients\n",
    "        \"\"\"\n",
    "        self.weight_infos[layer_id] = self.gamma*self.weight_infos[layer_id] + self.learning_rate*grads\n",
    "        self.intercept_infos[layer_id] = self.gamma*self.intercept_infos[layer_id] + self.learning_rate*intercept_grads\n",
    "        return self.weight_infos[layer_id], self.intercept_infos[layer_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDOptimizerAdagrad(GDOptimizer):\n",
    "    def __init__(self, learning_rate=0.01, epsilon=1e-8):\n",
    "        GDOptimizer.__init__(self)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def apply(self, layer_id, grads, intercept_grads):\n",
    "        # TODO\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDOptimizerAdadelta(GDOptimizer):\n",
    "    def __init__(self, gamma=0.9, epsilon=1e-8):\n",
    "        GDOptimizer.__init__(self)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def apply(self, layer_id, grads, intercept_grads):\n",
    "        # TODO\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Problems here, got a large error in the end and maybe not converged?\n",
    "class GDOptimizerRMSprop(GDOptimizer):\n",
    "    \"\"\"RMSprop.\n",
    "    \n",
    "    In RMSprop, weight_infos and intercept_infos stores the last calculated E[g^2].\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, gamma=0.9, epsilon=1e-8):\n",
    "        \"\"\"Init function.\n",
    "        \n",
    "        Attributes:\n",
    "            learning_rate (float): learning rate, default 0.001\n",
    "            gamma (float): rate of the update E[g^2] of the past time step to add on, default 0.9\n",
    "            epsilon (float):  smoothing term that avoids division by zero, default 1e-8\n",
    "        \"\"\"\n",
    "        GDOptimizer.__init__(self)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def apply(self, layer_id, grads, intercept_grads):\n",
    "        \"\"\"Apply optimizer to current gradients for update.\n",
    "        \n",
    "        Args:\n",
    "            layer_id (int/str): key to specify the layer\n",
    "            grads (np.array): gradients of weight\n",
    "            intercept_grads (np.array): gradients of intercept\n",
    "        \n",
    "        Returns:\n",
    "            E[g^2] = gamma * last_E[g^2] + (1-gamma) * current_gradients^2\n",
    "            learning_rate * gradients / sqrt(E[g^2] + epsilon)\n",
    "        \"\"\"\n",
    "        self.weight_infos[layer_id] = self.gamma*self.weight_infos[layer_id] \\\n",
    "            + (1-self.gamma)*np.power(grads,2)\n",
    "        self.intercept_infos[layer_id] = self.gamma*self.intercept_infos[layer_id] \\\n",
    "            + (1-self.gamma)*np.power(intercept_grads,2)\n",
    "        weight_delta = self.learning_rate * grads / np.sqrt(self.weight_infos[layer_id] + self.epsilon)\n",
    "        intercept_delta = self.learning_rate * intercept_grads / np.sqrt(self.intercept_infos[layer_id] + self.epsilon)\n",
    "        return weight_delta, intercept_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNLayer:\n",
    "    \"\"\"Single layer of neural network\"\"\"\n",
    "    \n",
    "    def __init__(self, node_count, has_intercepts, activation):\n",
    "        \"\"\"Init function.\n",
    "        \n",
    "        Attributes:\n",
    "            layer_id (int/str): ID of current layer\n",
    "            node_count (int): number of nodes of current layer\n",
    "            has_intercepts (boolean): flag if the nodes of the layer has intercept terms\n",
    "            activation (ActivationFunction): activation function\n",
    "            gd_optimizer (GDOptimizer): gradient descent optimizer for GD update\n",
    "            weights (np.array): input weights of all nodes\n",
    "            intercepts (np.array): intercepts of all nodes\n",
    "            weight_grads (np.array): weight gradients of all nodes of current step \n",
    "            intercept_grads (np.array): intercept gradients of all nodes of current step \n",
    "            step_input (np.array): input of current step\n",
    "            reduced_sum (np.array): reduced sum of input before activation of current step \n",
    "            step_output (np.array): output of current step\n",
    "        \"\"\"\n",
    "        self.layer_id = None\n",
    "        self.node_count = node_count\n",
    "        self.has_intercepts = has_intercepts\n",
    "        self.activation = activation\n",
    "        self.gd_optimizer = None\n",
    "        self.weights = None\n",
    "        self.intercepts = None\n",
    "        self.weight_grads = None\n",
    "        self.intercept_grads = None\n",
    "        self.step_input = None\n",
    "        self.reduced_sum = None\n",
    "        self.step_output = None\n",
    "    \n",
    "    def init(self, layer_id, prev_node_count, gd_optimizer):\n",
    "        \"\"\"Initialize layer info and nodes.\n",
    "        \n",
    "        Args:\n",
    "            layer_id (int/str): ID of current layer\n",
    "            prev_node_count (int): number of input of current layer\n",
    "            gd_optimizer (GDOptimizer): gradient descent optimizer for GD update\n",
    "        \"\"\"\n",
    "        self.layer_id = layer_id\n",
    "        self.gd_optimizer = gd_optimizer\n",
    "        self.weights, self.intercepts = self.xavier_weight_init(prev_node_count, self.node_count, self.has_intercepts)\n",
    "        self.gd_optimizer.register_layer(self.layer_id, self.weights.shape, self.intercepts.shape)\n",
    "    \n",
    "    def forward(self, step_input):\n",
    "        \"\"\"Run through the layer with given input to get output.\n",
    "        \n",
    "        Args:\n",
    "            step_input (np.array): input data\n",
    "        \n",
    "        Returns:\n",
    "            Result calculated by current layer\n",
    "            activation(input*weights+intercepts)\n",
    "        \"\"\"\n",
    "        self.step_input = step_input\n",
    "        self.reduced_sum = np.dot(self.step_input, self.weights)+self.intercepts\n",
    "        self.step_output = self.activation.apply(self.reduced_sum)\n",
    "        return self.step_output\n",
    "    \n",
    "    def back_propagation(self, prev_delta, activation_derivatived=False):\n",
    "        cur_delta = None\n",
    "        if activation_derivatived:\n",
    "            cur_delta = prev_delta\n",
    "        elif self.activation.derivative_use_activated:\n",
    "            cur_delta = prev_delta * self.activation.derivative(self.step_output)\n",
    "        else:\n",
    "            cur_delta = prev_delta * self.activation.derivative(self.reduced_sum)\n",
    "        self.weight_grads = np.dot(np.atleast_2d(self.step_input).T, cur_delta)/cur_delta.shape[0]\n",
    "        self.intercept_grads = cur_delta.mean(axis=0)\n",
    "        return np.dot(cur_delta, self.weights.T)\n",
    "    \n",
    "    def update_weights(self):\n",
    "        weight_deltas, intercept_deltas = \\\n",
    "            self.gd_optimizer.apply(self.layer_id, self.weight_grads, self.intercept_grads)\n",
    "        self.weights -= weight_deltas\n",
    "        if self.has_intercepts:\n",
    "            self.intercepts -= intercept_deltas\n",
    "    \n",
    "    def drop_out(self):\n",
    "        self.weights = np.zeros(self.weights.shape)\n",
    "        self.intercepts = np.zeros(self.intercepts.shape)\n",
    "        self.step_output = np.zeros(self.step_output.shape)\n",
    "    \n",
    "    def xavier_weight_init(self, prev_node_count, cur_node_count, has_intercepts):\n",
    "        weights = np.random.randn(prev_node_count, cur_node_count)/np.sqrt(prev_node_count)\n",
    "        intercepts = np.random.randn(cur_node_count)/np.sqrt(prev_node_count) \\\n",
    "            if has_intercepts else np.zeros(cur_node_count)\n",
    "        return weights, intercepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel:\n",
    "    def __init__(self, X_size, layers, gd_optimizer):\n",
    "        self.X_size = X_size\n",
    "        self.model = layers\n",
    "        self.gd_optimizer = gd_optimizer\n",
    "        self.init_model()\n",
    "    \n",
    "    # initialize model with layers\n",
    "    def init_model(self):\n",
    "        prev_node_count = self.X_size\n",
    "        for layer_no, layer in enumerate(self.model):\n",
    "            layer.init(layer_no, prev_node_count, self.gd_optimizer)\n",
    "            prev_node_count = layer.node_count\n",
    "    \n",
    "    # forward pass, calculate predict value with current model (batch or single)\n",
    "    def model_forward(self, X):\n",
    "        cur_res = X\n",
    "        for layer in self.model:\n",
    "            cur_res = layer.forward(cur_res)\n",
    "        return cur_res\n",
    "    \n",
    "    # update model with errs (y_predict - y) by back propagation\n",
    "    def update_model(self, y_predict, y):\n",
    "        delta = np.atleast_2d(y_predict - y)\n",
    "        for layer in reversed(self.model):\n",
    "            delta = layer.back_propagation(delta, activation_derivatived=(layer==self.model[-1]))\n",
    "        for layer in self.model:\n",
    "            layer.update_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = sklearn.datasets.load_breast_cancer(return_X_y=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_x_org = test_data['data']\n",
    "test_data_y_org = test_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_x_org.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_std = (test_data_x_org - np.mean(test_data_x_org, axis=0)) / np.std(test_data_x_org, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_x = test_data_std[:500]\n",
    "test_data_y = test_data_y_org[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    NNLayer(20, True, ActivationSigmoid(x_upper_bound=500, x_lower_bound=-500)),\n",
    "    NNLayer(10, True, ActivationSigmoid(x_upper_bound=500, x_lower_bound=-500)),\n",
    "    NNLayer(1, True, ActivationSigmoid(x_upper_bound=500, x_lower_bound=-500)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = NNModel(test_data_x.shape[1], layers, GDOptimizerNone(learning_rate=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_cost = []\n",
    "all_cost_ce = []\n",
    "for _ in range(rounds):\n",
    "    cost = 0\n",
    "    cost_ce = 0\n",
    "    learning_idx = np.arange(len(test_data_x))\n",
    "    np.random.shuffle(learning_idx)\n",
    "    \n",
    "    for start_idx in range(0, len(test_data_x), batch_size):\n",
    "        data_idx = learning_idx[start_idx : min(start_idx+batch_size,len(test_data_x))]\n",
    "#         data_idx = learning_idx[start_idx]\n",
    "        sample, target = test_data_x[data_idx], test_data_y[data_idx,None]\n",
    "        predict = m.model_forward(sample)\n",
    "        err = predict - target\n",
    "        m.update_model(predict, target)\n",
    "        cost += (err*err).sum()\n",
    "        cost_ce -= (target*np.log(predict)+(1-target)*np.log(1-predict)).sum()\n",
    "    all_cost.append(cost)\n",
    "    all_cost_ce.append(cost_ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003868889151943356"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cost[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09756259465177684"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cost_ce[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x5fb78d0>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x5f92ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(all_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data_x = test_data_std[500:]\n",
    "predict_data_y = test_data_y_org[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y = []\n",
    "for sample, tgt in zip(predict_data_x, predict_data_y):\n",
    "    pct = m.model_forward(sample)\n",
    "    predict_y.append(pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17,  0],\n",
       "       [ 2, 50]], dtype=int64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(predict_data_y, [1 if pct > 0.5 else 0 for pct in predict_y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
