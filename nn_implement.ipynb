{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(all='raise', under='warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn model define by layers:\n",
    "# (node_count, active_function)\n",
    "model_define = [\n",
    "#     (20, relu, relu_derivative),\n",
    "#     (env.action_space.n, sigmoid, sigmoid_derivative),\n",
    "    (20, None, None),\n",
    "    (env.action_space.n, None, None),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction(abc.ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def apply(self, v):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def derivative(self, v, activated_value):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationNone(ActivationFunction):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def apply(self, v):\n",
    "        return v\n",
    "    \n",
    "    def derivative(self, v, activated_value=True):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationSigmoid(ActivationFunction):\n",
    "    def __init__(self, x_upper_bound=None, x_lower_bound=None):\n",
    "        self.x_upper_bound = x_upper_bound\n",
    "        self.x_lower_bound = x_lower_bound\n",
    "    \n",
    "    def apply(self, v):\n",
    "        return 1.0 / (1.0 + np.exp(-v.clip(max=self.x_upper_bound, min=self.x_lower_bound)))\n",
    "    \n",
    "    def derivative(self, v, activated_value=True):\n",
    "        if not activated_value:\n",
    "            v = self.apply(v)\n",
    "        return v * (1 - v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationRelu(ActivationFunction):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def apply(self, v):\n",
    "        return v.clip(min=0)\n",
    "    \n",
    "    def derivative(self, v, activated_value=True):\n",
    "        if not activated_value:\n",
    "            v = self.apply(v)\n",
    "        return np.where(v>0,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNLayer:\n",
    "    def __init__(self, node_count, activation):\n",
    "        self.node_count = node_count\n",
    "        self.activation = activation\n",
    "        self.weights = None\n",
    "        self.interceptions = None\n",
    "    \n",
    "    def init_weights(self, prev_node_count, has_interception):\n",
    "        self.weights, self.interceptions = self.xavier_weight_init(prev_node_count, self.node_count, has_interception)\n",
    "    \n",
    "    def xavier_weight_init(self, prev_node_count, cur_node_count, has_interception):\n",
    "        weights = np.random.randn(prev_node_count, cur_node_count)/np.sqrt(prev_node_count)\n",
    "        interceptions = np.random.randn(cur_node_count)/np.sqrt(prev_node_count) \\\n",
    "            if has_interception else np.zeros(cur_node_count)\n",
    "        return weights, interceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel:\n",
    "    def __init__(self, X_size, layers, has_interception=True):\n",
    "        self.X_size = X_size\n",
    "        self.has_interception = has_interception\n",
    "        self.model = layers\n",
    "        self.hidden_layer_res_buf = []\n",
    "        self.init_model()\n",
    "    \n",
    "    def init_model(self):\n",
    "        prev_node_count = self.X_size\n",
    "        for layer in self.model:\n",
    "            layer.init_weights(prev_node_count, self.has_interception)\n",
    "            prev_node_count = layer.node_count\n",
    "    \n",
    "    # forward pass, calculate predict value with current model\n",
    "    def model_forward(self, X):\n",
    "        cur_res = X\n",
    "        self.hidden_layer_res_buf.clear()\n",
    "        for layer in self.model:\n",
    "            self.hidden_layer_res_buf.append(cur_res)\n",
    "            cur_res = np.dot(cur_res, layer.weights) + layer.interceptions\n",
    "            cur_res = layer.activation.apply(cur_res)\n",
    "        return cur_res\n",
    "    \n",
    "    # batch back propagation to get gradient\n",
    "    def back_propagation(self, td_err, learning_rate):\n",
    "        reversed_grads = []\n",
    "        delta = np.atleast_2d(td_err)\n",
    "        \n",
    "        for \n",
    "        \n",
    "        # TODO: bug here, layers mismatched during calculating delta with layer activation derivative\n",
    "        for layer_input, layer in zip(reversed(self.hidden_layer_res_buf), reversed(model)):\n",
    "            reversed_grads.append((np.dot(np.atleast_2d(layer_input).T, delta)/delta.shape[0],\n",
    "                                   delta.mean(axis=0)))\n",
    "            delta = np.dot(delta, layer.weights.T) * layer.activation.derivative(layer_input)\n",
    "        self.update_model(reversed(reversed_grads), learning_rate)\n",
    "    \n",
    "    # update model\n",
    "    def update_model(self, grads, learning_rate)\n",
    "        for i in range(len(self.model)):\n",
    "            self.model[i][0] = self.model[i][0] + learning_rate * grads[i][0]\n",
    "            self.model[i][1] = self.model[i][1] + learning_rate * grads[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
