{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_settings = np.seterr(all='warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'warn'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr(all='raise', under='warn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deep Q model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "def model_init(input_size, model_define):\n",
    "    model.clear()\n",
    "    prev_node_count = input_size\n",
    "    for node_count, activation_func, activation_derivative in model_define:\n",
    "        model.append([np.random.randn(prev_node_count, node_count)/np.sqrt(prev_node_count),\n",
    "                      np.random.randn(node_count)/np.sqrt(prev_node_count),\n",
    "                      activation_func,\n",
    "                      activation_derivative])\n",
    "        prev_node_count = node_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass, calculate predict value with current model\n",
    "def model_forward(state):\n",
    "    cur_res = state\n",
    "    hidden_layer_input_buf = []\n",
    "    for layer_weight, inter_weight, activation_func, _ in model:\n",
    "        hidden_layer_input_buf.append(cur_res)\n",
    "        cur_res = np.dot(cur_res, layer_weight)\n",
    "        cur_res += inter_weight\n",
    "        if activation_func:\n",
    "            cur_res = activation_func(cur_res)\n",
    "    return cur_res, hidden_layer_input_buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back propagation to update model\n",
    "def back_propagation(td_err, hidden_layer_input_buf):\n",
    "    grads = []\n",
    "    delta = td_err\n",
    "    for layer_out, (layer_weight, inter_weight, _, activation_derivative) \\\n",
    "        in zip(reversed(hidden_layer_input_buf), reversed(model)):\n",
    "        grads.append((np.outer(layer_out.T, delta), delta))\n",
    "        delta = np.dot(layer_weight, delta)\n",
    "        if activation_derivative:\n",
    "            delta = delta * activation_derivative(layer_out)\n",
    "    grads.reverse()\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update model with gradients\n",
    "def update_model(grads):\n",
    "    for i in range(len(model)):\n",
    "        model[i][0] = model[i][0] + learning_rate * grads[i][0]\n",
    "        model[i][1] = model[i][1] + learning_rate * grads[i][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'FrozenLake-v0'\n",
    "# env_name = 'FrozenLakeNotSlippery-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "behavior define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary expand categorical type to preprocess states\n",
    "def binary_expand(n, idx=env.observation_space.n):\n",
    "    if type(idx) is int:\n",
    "        idx = np.array(list(range(idx)))\n",
    "    res = np.zeros(idx.shape)\n",
    "    res[n==idx] = 1\n",
    "    return res\n",
    "\n",
    "preprocess_func = binary_expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active function: sigmoid\n",
    "def sigmoid(v):\n",
    "    return 1.0 / (1.0 + np.exp(-v.clip(max=500,min=-500)))\n",
    "\n",
    "def sigmoid_derivative(sig_v):\n",
    "    return sig_v * (1 - sig_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active function: ReLU\n",
    "def relu(v):\n",
    "    return v.clip(min=0)\n",
    "\n",
    "def relu_derivative(v):\n",
    "    return np.where(v>0,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn model define by layers:\n",
    "# (node_count, active_function)\n",
    "model_define = [\n",
    "#     (20, relu, relu_derivative),\n",
    "#     (env.action_space.n, sigmoid, sigmoid_derivative),\n",
    "    (20, None, None),\n",
    "    (env.action_space.n, None, None),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 30000\n",
    "learning_rate = 0.03\n",
    "discount = 0.99\n",
    "\n",
    "max_epsilon = 1\n",
    "min_epsilon = 0.01\n",
    "epsilon_decay = 0.001\n",
    "epsilon = max_epsilon\n",
    "\n",
    "print_step = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess state information into input parameters\n",
    "preprocess_state = preprocess_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_init(env.observation_space.n, model_define)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In episode 1000, avg 12.29 steps are used, avg total reward is 0.046.\n",
      "In episode 2000, avg 20.331 steps are used, avg total reward is 0.161.\n",
      "In episode 3000, avg 32.064 steps are used, avg total reward is 0.377.\n",
      "In episode 4000, avg 39.034 steps are used, avg total reward is 0.532.\n",
      "In episode 5000, avg 41.24 steps are used, avg total reward is 0.654.\n",
      "In episode 6000, avg 43.841 steps are used, avg total reward is 0.658.\n",
      "In episode 7000, avg 42.759 steps are used, avg total reward is 0.664.\n",
      "In episode 8000, avg 42.315 steps are used, avg total reward is 0.671.\n",
      "In episode 9000, avg 43.171 steps are used, avg total reward is 0.676.\n",
      "In episode 10000, avg 44.142 steps are used, avg total reward is 0.678.\n",
      "In episode 11000, avg 42.934 steps are used, avg total reward is 0.674.\n",
      "In episode 12000, avg 44.728 steps are used, avg total reward is 0.667.\n",
      "In episode 13000, avg 43.965 steps are used, avg total reward is 0.666.\n",
      "In episode 14000, avg 43.069 steps are used, avg total reward is 0.665.\n",
      "In episode 15000, avg 41.553 steps are used, avg total reward is 0.703.\n",
      "In episode 16000, avg 43.655 steps are used, avg total reward is 0.685.\n",
      "In episode 17000, avg 43.332 steps are used, avg total reward is 0.681.\n",
      "In episode 18000, avg 44.022 steps are used, avg total reward is 0.654.\n",
      "In episode 19000, avg 41.139 steps are used, avg total reward is 0.702.\n",
      "In episode 20000, avg 42.19 steps are used, avg total reward is 0.691.\n",
      "In episode 21000, avg 43.721 steps are used, avg total reward is 0.67.\n",
      "In episode 22000, avg 42.68 steps are used, avg total reward is 0.683.\n",
      "In episode 23000, avg 43.347 steps are used, avg total reward is 0.684.\n",
      "In episode 24000, avg 43.391 steps are used, avg total reward is 0.663.\n",
      "In episode 25000, avg 42.991 steps are used, avg total reward is 0.726.\n",
      "In episode 26000, avg 44.476 steps are used, avg total reward is 0.684.\n",
      "In episode 27000, avg 45.234 steps are used, avg total reward is 0.681.\n",
      "In episode 28000, avg 42.656 steps are used, avg total reward is 0.702.\n",
      "In episode 29000, avg 42.139 steps are used, avg total reward is 0.705.\n",
      "In episode 30000, avg 43.665 steps are used, avg total reward is 0.668.\n"
     ]
    }
   ],
   "source": [
    "step_count_list = []\n",
    "total_reward_list = []\n",
    "reward_ob = []\n",
    "for ep in range(1, episodes+1):\n",
    "    end = False\n",
    "    step_count = 0\n",
    "    total_reward = 0\n",
    "    reward_list = []\n",
    "    \n",
    "    # startup state\n",
    "    new_state = preprocess_state(env.reset())\n",
    "    \n",
    "    # run until game end\n",
    "    while not end:\n",
    "        # predict with the latest model\n",
    "        state = new_state\n",
    "        q_values, hidden_layer_input_buf = model_forward(state)\n",
    "        \n",
    "        # epsilon-greedy action selection\n",
    "        if np.random.rand() > epsilon:\n",
    "            action = np.argmax(q_values)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # step forward\n",
    "        new_state_no, reward, end, _ = env.step(action)\n",
    "        step_count += 1\n",
    "        total_reward += reward\n",
    "        \n",
    "        # update q values with actual returns\n",
    "        # save new state for the next step\n",
    "        new_state = preprocess_state(new_state_no)\n",
    "        \n",
    "        # calculate error for back propagtion with Bellman equation\n",
    "        td_err = np.zeros_like(q_values)\n",
    "        next_q_values, _ = model_forward(new_state)\n",
    "        td_err[action] = reward + discount * np.max(next_q_values) * (not end) - q_values[action]\n",
    "        \n",
    "        reward_list.append((reward, td_err))\n",
    "        \n",
    "        # back propagation with td error\n",
    "        grads = back_propagation(td_err, hidden_layer_input_buf)\n",
    "        \n",
    "        # update model with gradients\n",
    "        update_model(grads)\n",
    "        \n",
    "    reward_ob.append(reward_list)\n",
    "    \n",
    "    # update epsilon\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-epsilon_decay*ep)\n",
    "    \n",
    "    # record step counts\n",
    "    step_count_list.append(step_count)\n",
    "    total_reward_list.append(total_reward)\n",
    "    # print informations\n",
    "    if (ep)%print_step == 0:\n",
    "        print('In episode {}, avg {} steps are used, avg total reward is {}.'.format(\n",
    "            ep, sum(step_count_list)/print_step, sum(total_reward_list)/print_step))\n",
    "        step_count_list.clear()\n",
    "        total_reward_list.clear()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "end = False\n",
    "state = preprocess_state(env.reset())\n",
    "env.render()\n",
    "while not end:\n",
    "    q_values, _ = model_forward(state)\n",
    "    action = np.argmax(q_values)\n",
    "    state_no, _, end, _ = env.step(action)\n",
    "    env.render()\n",
    "    state = preprocess_state(state_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5630969  0.50566662 0.46317781 0.47563946]\n",
      "[0.4106695  0.40228291 0.30889418 0.48346134]\n",
      "[0.46073009 0.38687891 0.34426882 0.45141782]\n",
      "[0.39939655 0.31686998 0.29295624 0.43066903]\n",
      "[0.58253161 0.36637333 0.34773745 0.33504698]\n",
      "[0.46218707 0.41620034 0.54831936 0.57040386]\n",
      "[0.45226855 0.29203177 0.29349483 0.20992347]\n",
      "[0.41671437 0.6447355  0.86118689 0.82397516]\n",
      "[0.40371493 0.44696051 0.44072058 0.61950365]\n",
      "[0.50545381 0.69777568 0.47948539 0.44467505]\n",
      "[0.69990072 0.53748153 0.43469887 0.36598947]\n",
      "[0.50282117 0.32354417 0.35526406 0.16985848]\n",
      "[0.64575089 0.51970965 0.52211683 0.16347403]\n",
      "[0.55105429 0.58008346 0.74570892 0.48268673]\n",
      "[0.69054793 0.91129654 0.77078684 0.77018467]\n",
      "[0.59322792 0.64340212 0.58019897 0.48540829]\n"
     ]
    }
   ],
   "source": [
    "for i in range(16):\n",
    "    q_values, _ = model_forward(preprocess_state(i))\n",
    "    print(q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test nn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_define_test = [\n",
    "    (20, relu, relu_derivative),\n",
    "#     (env.action_space.n, sigmoid, sigmoid_derivative),\n",
    "    (env.action_space.n, None, None),\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = []\n",
    "prev_node_count = env.observation_space.n\n",
    "for node_count, activation_func, activation_derivative in model_define_test:\n",
    "    model.append([np.random.randn(prev_node_count, node_count)/np.sqrt(prev_node_count),\n",
    "                  activation_func,\n",
    "                  activation_derivative])\n",
    "    prev_node_count = node_count"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rounds = 10000"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for _ in range(rounds):\n",
    "    for state_no in range(16):\n",
    "        test_state = binary_expand(state_no)\n",
    "        tgt_action = state_no%4\n",
    "        \n",
    "        q_values = model_forward(test_state)\n",
    "\n",
    "        rewards = np.zeros_like(q_values)\n",
    "        rewards[tgt_action] = 1\n",
    "        rewards[(tgt_action+1)%4] = 1\n",
    "        td_err = rewards - q_values\n",
    "        # td_err\n",
    "\n",
    "        grads = back_propagation(td_err)\n",
    "\n",
    "        update_model(grads)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(16):\n",
    "    print(model_forward(preprocess_state(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test nn with sklearn.datasets.load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
