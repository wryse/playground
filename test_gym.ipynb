{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_settings = np.seterr(all='warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'warn'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr(all='raise', under='warn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'FrozenLake-v0'\n",
    "# env_name = 'FrozenLakeNotSlippery-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "behavior define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary expand categorical type to preprocess states\n",
    "def binary_expand(n, idx=env.observation_space.n):\n",
    "    if type(idx) is int:\n",
    "        idx = np.array(list(range(idx)))\n",
    "    res = np.zeros(idx.shape)\n",
    "    res[n==idx] = 1\n",
    "    return res\n",
    "\n",
    "preprocess_func = binary_expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active function: sigmoid\n",
    "def sigmoid(v):\n",
    "    return 1.0 / (1.0 + np.exp(-v.clip(max=500,min=-500)))\n",
    "\n",
    "def sigmoid_derivative(v):\n",
    "    return v * (1 - v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active function: ReLU\n",
    "def relu(v):\n",
    "    return v.clip(min=0)\n",
    "\n",
    "def relu_derivative(v):\n",
    "    return np.where(v>0,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn model define by layers:\n",
    "# (node_count, active_function)\n",
    "model_define = [\n",
    "#     (20, relu, relu_derivative),\n",
    "#     (env.action_space.n, sigmoid, sigmoid_derivative),\n",
    "    (50, None, None),\n",
    "    (env.action_space.n, None, None),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 30000\n",
    "learning_rate = 0.03\n",
    "discount = 0.99\n",
    "\n",
    "max_epsilon = 1\n",
    "min_epsilon = 0.01\n",
    "epsilon_decay = 0.001\n",
    "epsilon = max_epsilon\n",
    "\n",
    "print_step = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deep Q engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess state information into input parameters\n",
    "preprocess_state = preprocess_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass, calculate predict value with current model\n",
    "def model_forward(state):\n",
    "    cur_res = state\n",
    "    hidden_layer_input_buf = []\n",
    "    for layer, activation_func, _ in model:\n",
    "        hidden_layer_input_buf.append(cur_res)\n",
    "        cur_res = np.dot(cur_res, layer)\n",
    "        if activation_func:\n",
    "            cur_res = activation_func(cur_res)\n",
    "    return cur_res, hidden_layer_input_buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back propagation to update model\n",
    "def back_propagation(td_err, hidden_layer_input_buf):\n",
    "    grads = []\n",
    "    delta = td_err\n",
    "    for layer_out, (layer, _, activation_derivative) in zip(reversed(hidden_layer_input_buf), reversed(model)):\n",
    "        grads.append(np.outer(layer_out.T, delta))\n",
    "        delta = np.dot(layer, delta)\n",
    "        if activation_derivative:\n",
    "            delta = delta * activation_derivative(layer_out)\n",
    "    grads.reverse()\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update model with gradients\n",
    "def update_model(grads):\n",
    "    for i in range(len(model)):\n",
    "        model[i][0] = model[i][0] + learning_rate * grads[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = []\n",
    "prev_node_count = env.observation_space.n\n",
    "for node_count, activation_func, activation_derivative in model_define:\n",
    "    model.append([np.random.randn(prev_node_count, node_count)/np.sqrt(prev_node_count),\n",
    "                  activation_func,\n",
    "                  activation_derivative])\n",
    "    prev_node_count = node_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In episode 1000, avg 11.309 steps are used, avg total reward is 0.045.\n",
      "In episode 2000, avg 21.502 steps are used, avg total reward is 0.205.\n",
      "In episode 3000, avg 32.013 steps are used, avg total reward is 0.409.\n",
      "In episode 4000, avg 35.698 steps are used, avg total reward is 0.564.\n",
      "In episode 5000, avg 39.175 steps are used, avg total reward is 0.597.\n",
      "In episode 6000, avg 41.136 steps are used, avg total reward is 0.647.\n",
      "In episode 7000, avg 41.735 steps are used, avg total reward is 0.635.\n",
      "In episode 8000, avg 38.513 steps are used, avg total reward is 0.676.\n",
      "In episode 9000, avg 42.399 steps are used, avg total reward is 0.686.\n",
      "In episode 10000, avg 42.367 steps are used, avg total reward is 0.628.\n",
      "In episode 11000, avg 43.11 steps are used, avg total reward is 0.66.\n",
      "In episode 12000, avg 43.52 steps are used, avg total reward is 0.657.\n",
      "In episode 13000, avg 41.509 steps are used, avg total reward is 0.614.\n",
      "In episode 14000, avg 36.874 steps are used, avg total reward is 0.65.\n",
      "In episode 15000, avg 38.707 steps are used, avg total reward is 0.676.\n",
      "In episode 16000, avg 37.425 steps are used, avg total reward is 0.671.\n",
      "In episode 17000, avg 39.976 steps are used, avg total reward is 0.667.\n",
      "In episode 18000, avg 41.009 steps are used, avg total reward is 0.682.\n",
      "In episode 19000, avg 43.746 steps are used, avg total reward is 0.649.\n",
      "In episode 20000, avg 42.694 steps are used, avg total reward is 0.638.\n",
      "In episode 21000, avg 42.673 steps are used, avg total reward is 0.661.\n",
      "In episode 22000, avg 40.687 steps are used, avg total reward is 0.673.\n",
      "In episode 23000, avg 45.251 steps are used, avg total reward is 0.618.\n",
      "In episode 24000, avg 43.606 steps are used, avg total reward is 0.661.\n",
      "In episode 25000, avg 41.716 steps are used, avg total reward is 0.664.\n",
      "In episode 26000, avg 39.737 steps are used, avg total reward is 0.666.\n",
      "In episode 27000, avg 39.159 steps are used, avg total reward is 0.658.\n",
      "In episode 28000, avg 38.495 steps are used, avg total reward is 0.684.\n",
      "In episode 29000, avg 38.904 steps are used, avg total reward is 0.674.\n",
      "In episode 30000, avg 42.938 steps are used, avg total reward is 0.657.\n"
     ]
    }
   ],
   "source": [
    "step_count_list = []\n",
    "total_reward_list = []\n",
    "reward_ob = []\n",
    "for ep in range(1, episodes+1):\n",
    "    end = False\n",
    "    step_count = 0\n",
    "    total_reward = 0\n",
    "    reward_list = []\n",
    "    \n",
    "    # startup state\n",
    "    new_state = preprocess_state(env.reset())\n",
    "    \n",
    "    # run until game end\n",
    "    while not end:\n",
    "        # predict with the latest model\n",
    "        state = new_state\n",
    "        q_values, hidden_layer_input_buf = model_forward(state)\n",
    "        \n",
    "        # epsilon-greedy action selection\n",
    "        if np.random.rand() > epsilon:\n",
    "            action = np.argmax(q_values)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # step forward\n",
    "        new_state_no, reward, end, _ = env.step(action)\n",
    "        step_count += 1\n",
    "        total_reward += reward\n",
    "        \n",
    "        # update q values with actual returns\n",
    "        # save new state for the next step\n",
    "        new_state = preprocess_state(new_state_no)\n",
    "        \n",
    "        # calculate error for back propagtion with Bellman equation\n",
    "        td_err = np.zeros_like(q_values)\n",
    "        next_q_values, _ = model_forward(new_state)\n",
    "        td_err[action] = reward + discount * np.max(next_q_values) * (not end) - q_values[action]\n",
    "        \n",
    "        reward_list.append((reward, td_err))\n",
    "        \n",
    "        # back propagation with td error\n",
    "        grads = back_propagation(td_err, hidden_layer_input_buf)\n",
    "        \n",
    "        # update model with gradients\n",
    "        update_model(grads)\n",
    "        \n",
    "    reward_ob.append(reward_list)\n",
    "    \n",
    "    # update epsilon\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-epsilon_decay*ep)\n",
    "    \n",
    "    # record step counts\n",
    "    step_count_list.append(step_count)\n",
    "    total_reward_list.append(total_reward)\n",
    "    # print informations\n",
    "    if (ep)%print_step == 0:\n",
    "        print('In episode {}, avg {} steps are used, avg total reward is {}.'.format(\n",
    "            ep, sum(step_count_list)/print_step, sum(total_reward_list)/print_step))\n",
    "        step_count_list.clear()\n",
    "        total_reward_list.clear()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "end = False\n",
    "state = preprocess_state(env.reset())\n",
    "env.render()\n",
    "while not end:\n",
    "    q_values, _ = model_forward(state)\n",
    "    action = np.argmax(q_values)\n",
    "    state_no, _, end, _ = env.step(action)\n",
    "    env.render()\n",
    "    state = preprocess_state(state_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.44176123 0.39228165 0.39312355 0.38193445]\n",
      "[0.29775156 0.31812821 0.26201089 0.37133117]\n",
      "[0.31341036 0.25036357 0.26266108 0.34567234]\n",
      "[0.1556511  0.218531   0.20644806 0.33812346]\n",
      "[0.45459239 0.40034054 0.37292717 0.28068976]\n",
      "[ 0.0250409  -0.04699562  0.12119069  0.30352165]\n",
      "[0.17927158 0.13745011 0.24337014 0.07157744]\n",
      "[0.19761918 0.26500022 0.12002214 0.2486737 ]\n",
      "[0.3059476  0.4097947  0.32701651 0.5060742 ]\n",
      "[0.47312837 0.61231524 0.43795595 0.38931995]\n",
      "[0.5512142  0.40252655 0.3467512  0.27015182]\n",
      "[0.08836059 0.02600961 0.00816638 0.22208995]\n",
      "[ 0.05163072 -0.00270992  0.11372     0.3796335 ]\n",
      "[0.39521913 0.49691806 0.72268208 0.50598365]\n",
      "[0.62918252 0.83175076 0.71998892 0.72762153]\n",
      "[ 0.08592947 -0.21484125  0.05120207  0.01428415]\n"
     ]
    }
   ],
   "source": [
    "for i in range(16):\n",
    "    q_values, _ = model_forward(preprocess_state(i))\n",
    "    print(q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test nn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_define_test = [\n",
    "    (20, relu, relu_derivative),\n",
    "#     (env.action_space.n, sigmoid, sigmoid_derivative),\n",
    "    (env.action_space.n, None, None),\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = []\n",
    "prev_node_count = env.observation_space.n\n",
    "for node_count, activation_func, activation_derivative in model_define_test:\n",
    "    model.append([np.random.randn(prev_node_count, node_count)/np.sqrt(prev_node_count),\n",
    "                  activation_func,\n",
    "                  activation_derivative])\n",
    "    prev_node_count = node_count"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rounds = 10000"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for _ in range(rounds):\n",
    "    for state_no in range(16):\n",
    "        test_state = binary_expand(state_no)\n",
    "        tgt_action = state_no%4\n",
    "        \n",
    "        q_values = model_forward(test_state)\n",
    "\n",
    "        rewards = np.zeros_like(q_values)\n",
    "        rewards[tgt_action] = 1\n",
    "        rewards[(tgt_action+1)%4] = 1\n",
    "        td_err = rewards - q_values\n",
    "        # td_err\n",
    "\n",
    "        grads = back_propagation(td_err)\n",
    "\n",
    "        update_model(grads)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(16):\n",
    "    print(model_forward(preprocess_state(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
