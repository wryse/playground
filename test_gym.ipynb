{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_settings = np.seterr(all='warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'warn'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr(all='raise', under='warn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deep Q model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "def model_init(input_size, model_define):\n",
    "    model.clear()\n",
    "    prev_node_count = input_size\n",
    "    for node_count, activation_func, activation_derivative in model_define:\n",
    "        model.append([np.random.randn(prev_node_count, node_count)/np.sqrt(prev_node_count),\n",
    "                      np.random.randn(node_count)/np.sqrt(prev_node_count),\n",
    "                      activation_func,\n",
    "                      activation_derivative])\n",
    "        prev_node_count = node_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch forward pass, calculate predict value with current model\n",
    "def model_forward(states):\n",
    "    cur_res = np.atleast_2d(states)\n",
    "    hidden_layer_input_buf = []\n",
    "    for layer_weight, inter_weight, activation_func, _ in model:\n",
    "        hidden_layer_input_buf.append(cur_res)\n",
    "        cur_res = np.dot(cur_res, layer_weight)\n",
    "        cur_res += inter_weight\n",
    "        if activation_func:\n",
    "            cur_res = activation_func(cur_res)\n",
    "    return cur_res, hidden_layer_input_buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch back propagation to update model\n",
    "def back_propagation(td_err, hidden_layer_input_buf):\n",
    "    grads = []\n",
    "    delta = np.atleast_2d(td_err)\n",
    "    for layer_out, (layer_weight, inter_weight, _, activation_derivative) \\\n",
    "        in zip(reversed(hidden_layer_input_buf), reversed(model)):\n",
    "        grads.append((np.dot(np.atleast_2d(layer_out.T), delta)/delta.shape[0],\n",
    "                      delta.mean(axis=0)))\n",
    "        delta = np.dot(delta, layer_weight.T)\n",
    "        if activation_derivative:\n",
    "            delta = delta * activation_derivative(layer_out)\n",
    "    grads.reverse()\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update model with gradients\n",
    "def update_model(grads):\n",
    "    for i in range(len(model)):\n",
    "        model[i][0] = model[i][0] + learning_rate * grads[i][0]\n",
    "        model[i][1] = model[i][1] + learning_rate * grads[i][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'FrozenLake-v0'\n",
    "# env_name = 'FrozenLakeNotSlippery-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "behavior define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary expand categorical type to preprocess states\n",
    "def binary_expand(n, idx=env.observation_space.n):\n",
    "    if type(idx) is int:\n",
    "        idx = np.array(list(range(idx)))\n",
    "    res = np.zeros(idx.shape)\n",
    "    res[n==idx] = 1\n",
    "    return res\n",
    "\n",
    "preprocess_func = binary_expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_expand_mod(n, idx=env.observation_space.n):\n",
    "    if type(idx) is int:\n",
    "        idx = np.array(list(range(idx)))\n",
    "    return np.identity(len(idx))[np.where(n==idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active function: sigmoid\n",
    "def sigmoid(v):\n",
    "    return 1.0 / (1.0 + np.exp(-v.clip(max=500,min=-500)))\n",
    "\n",
    "def sigmoid_derivative(sig_v):\n",
    "    return sig_v * (1 - sig_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active function: ReLU\n",
    "def relu(v):\n",
    "    return v.clip(min=0)\n",
    "\n",
    "def relu_derivative(v):\n",
    "    return np.where(v>0,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn model define by layers:\n",
    "# (node_count, active_function)\n",
    "model_define = [\n",
    "#     (20, relu, relu_derivative),\n",
    "#     (env.action_space.n, sigmoid, sigmoid_derivative),\n",
    "    (20, None, None),\n",
    "    (env.action_space.n, None, None),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 30000\n",
    "learning_rate = 0.03\n",
    "discount = 0.99\n",
    "\n",
    "max_epsilon = 1\n",
    "min_epsilon = 0.01\n",
    "epsilon_decay = 0.001\n",
    "epsilon = max_epsilon\n",
    "\n",
    "print_step = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess state information into input parameters\n",
    "preprocess_state = preprocess_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_init(env.observation_space.n, model_define)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In episode 1000, avg 11.063 steps are used, avg total reward is 0.036.\n",
      "In episode 2000, avg 20.939 steps are used, avg total reward is 0.183.\n",
      "In episode 3000, avg 31.276 steps are used, avg total reward is 0.4.\n",
      "In episode 4000, avg 40.734 steps are used, avg total reward is 0.548.\n",
      "In episode 5000, avg 40.819 steps are used, avg total reward is 0.656.\n",
      "In episode 6000, avg 41.39 steps are used, avg total reward is 0.649.\n",
      "In episode 7000, avg 43.764 steps are used, avg total reward is 0.654.\n",
      "In episode 8000, avg 43.723 steps are used, avg total reward is 0.672.\n",
      "In episode 9000, avg 42.275 steps are used, avg total reward is 0.666.\n",
      "In episode 10000, avg 42.576 steps are used, avg total reward is 0.696.\n",
      "In episode 11000, avg 42.85 steps are used, avg total reward is 0.671.\n",
      "In episode 12000, avg 43.634 steps are used, avg total reward is 0.682.\n",
      "In episode 13000, avg 42.544 steps are used, avg total reward is 0.685.\n",
      "In episode 14000, avg 43.188 steps are used, avg total reward is 0.692.\n",
      "In episode 15000, avg 43.262 steps are used, avg total reward is 0.681.\n",
      "In episode 16000, avg 41.809 steps are used, avg total reward is 0.685.\n",
      "In episode 17000, avg 42.703 steps are used, avg total reward is 0.669.\n",
      "In episode 18000, avg 43.643 steps are used, avg total reward is 0.68.\n",
      "In episode 19000, avg 42.954 steps are used, avg total reward is 0.673.\n",
      "In episode 20000, avg 42.913 steps are used, avg total reward is 0.661.\n",
      "In episode 21000, avg 43.201 steps are used, avg total reward is 0.674.\n",
      "In episode 22000, avg 41.034 steps are used, avg total reward is 0.68.\n",
      "In episode 23000, avg 42.961 steps are used, avg total reward is 0.695.\n",
      "In episode 24000, avg 43.926 steps are used, avg total reward is 0.672.\n",
      "In episode 25000, avg 42.563 steps are used, avg total reward is 0.676.\n",
      "In episode 26000, avg 42.929 steps are used, avg total reward is 0.668.\n",
      "In episode 27000, avg 42.458 steps are used, avg total reward is 0.673.\n",
      "In episode 28000, avg 42.803 steps are used, avg total reward is 0.694.\n",
      "In episode 29000, avg 43.368 steps are used, avg total reward is 0.68.\n",
      "In episode 30000, avg 41.583 steps are used, avg total reward is 0.679.\n"
     ]
    }
   ],
   "source": [
    "step_count_list = []\n",
    "total_reward_list = []\n",
    "reward_ob = []\n",
    "for ep in range(1, episodes+1):\n",
    "    end = False\n",
    "    step_count = 0\n",
    "    total_reward = 0\n",
    "    reward_list = []\n",
    "    \n",
    "    # startup state\n",
    "    new_state = preprocess_state(env.reset())\n",
    "    \n",
    "    # run until game end\n",
    "    while not end:\n",
    "        # predict with the latest model\n",
    "        state = new_state\n",
    "        q_values, hidden_layer_input_buf = model_forward(state)\n",
    "        \n",
    "        # epsilon-greedy action selection\n",
    "        if np.random.rand() > epsilon:\n",
    "            action = np.argmax(q_values)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # step forward\n",
    "        new_state_no, reward, end, _ = env.step(action)\n",
    "        step_count += 1\n",
    "        total_reward += reward\n",
    "        \n",
    "        # update q values with actual returns\n",
    "        # save new state for the next step\n",
    "        new_state = preprocess_state(new_state_no)\n",
    "        \n",
    "        # calculate error for back propagtion with Bellman equation\n",
    "        # TODO: need to modify error calculation to apply batch functions\n",
    "        td_err = np.zeros_like(q_values)\n",
    "        next_q_values, _ = model_forward(new_state)\n",
    "        td_err[0][action] = reward + discount * np.max(next_q_values) * (not end) - q_values[0][action]\n",
    "        \n",
    "        reward_list.append((reward, td_err))\n",
    "        \n",
    "        # back propagation with td error\n",
    "        grads = back_propagation(td_err, hidden_layer_input_buf)\n",
    "        \n",
    "        # update model with gradients\n",
    "        update_model(grads)\n",
    "        \n",
    "    reward_ob.append(reward_list)\n",
    "    \n",
    "    # update epsilon\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-epsilon_decay*ep)\n",
    "    \n",
    "    # record step counts\n",
    "    step_count_list.append(step_count)\n",
    "    total_reward_list.append(total_reward)\n",
    "    # print informations\n",
    "    if (ep)%print_step == 0:\n",
    "        print('In episode {}, avg {} steps are used, avg total reward is {}.'.format(\n",
    "            ep, sum(step_count_list)/print_step, sum(total_reward_list)/print_step))\n",
    "        step_count_list.clear()\n",
    "        total_reward_list.clear()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "end = False\n",
    "state = preprocess_state(env.reset())\n",
    "env.render()\n",
    "while not end:\n",
    "    q_values, _ = model_forward(state)\n",
    "    action = np.argmax(q_values)\n",
    "    state_no, _, end, _ = env.step(action)\n",
    "    env.render()\n",
    "    state = preprocess_state(state_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.51473984 0.48080445 0.42777855 0.47582202]\n",
      " [0.3368716  0.38301985 0.31254668 0.4699393 ]\n",
      " [0.36348846 0.36301077 0.31556663 0.4428129 ]\n",
      " [0.31522579 0.3370004  0.22888494 0.41972735]\n",
      " [0.53462124 0.37436321 0.37029661 0.3415028 ]\n",
      " [0.33410536 0.67124027 0.44401701 0.44399237]\n",
      " [0.25783028 0.25147451 0.35105163 0.11138381]\n",
      " [0.37685194 0.44877215 0.33855867 0.39988035]\n",
      " [0.33778966 0.43612407 0.37178573 0.58912297]\n",
      " [0.43508694 0.64265339 0.4294603  0.3711778 ]\n",
      " [0.60241005 0.52463266 0.39626051 0.33004984]\n",
      " [0.40077839 0.51634073 0.28478198 0.39648419]\n",
      " [0.51114158 0.36385602 0.43240276 0.55760059]\n",
      " [0.40719497 0.58140697 0.71125142 0.38172757]\n",
      " [0.6233313  0.89722137 0.73812503 0.75215066]\n",
      " [0.60184097 0.64187536 0.42983917 0.46089344]]\n"
     ]
    }
   ],
   "source": [
    "q_values, _ = model_forward(np.identity(16))\n",
    "print(q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test nn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_define_test = [\n",
    "    (20, relu, relu_derivative),\n",
    "#     (env.action_space.n, sigmoid, sigmoid_derivative),\n",
    "    (env.action_space.n, None, None),\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = []\n",
    "prev_node_count = env.observation_space.n\n",
    "for node_count, activation_func, activation_derivative in model_define_test:\n",
    "    model.append([np.random.randn(prev_node_count, node_count)/np.sqrt(prev_node_count),\n",
    "                  activation_func,\n",
    "                  activation_derivative])\n",
    "    prev_node_count = node_count"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rounds = 10000"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for _ in range(rounds):\n",
    "    for state_no in range(16):\n",
    "        test_state = binary_expand(state_no)\n",
    "        tgt_action = state_no%4\n",
    "        \n",
    "        q_values = model_forward(test_state)\n",
    "\n",
    "        rewards = np.zeros_like(q_values)\n",
    "        rewards[tgt_action] = 1\n",
    "        rewards[(tgt_action+1)%4] = 1\n",
    "        td_err = rewards - q_values\n",
    "        # td_err\n",
    "\n",
    "        grads = back_propagation(td_err)\n",
    "\n",
    "        update_model(grads)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(16):\n",
    "    print(model_forward(preprocess_state(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test nn with sklearn.datasets.load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
